# 优化器

## 概述：

**优化器在DL中起着灯塔的作用，指引参数往正确的方向更新，一个好的优化器能够使模型收敛速度加快，跳出局部最优，训练过程平稳等**



## 定义：

<div align=center>
<img src="https://note.youdao.com/yws/api/personal/file/WEBf3a493a26f0ca682c28490cbeed56047?method=download&shareKey=633e502b6430ba0aa9190e6033dcd060"/>
</div>



## SGD

<div align=center>
<img src="https://note.youdao.com/yws/api/personal/file/WEB48bd81fb3168f7cb81f030c06f3773cc?method=download&shareKey=c998f266a9b90d1d3c6317fdda4fbb19"/>
</div>



#### 优点：

- **SGD法由于每次仅仅采用mini-batch来迭代，计算速度很快。**

#### 缺点：

- **下降速度慢，训练容易震荡**
- **可能陷入局部最优点**



# SGDM：SGD with Nesterov Moment 

**为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性即历史时刻的梯度信息。**

<div align=center>
<img src="https://note.youdao.com/yws/api/personal/file/WEBd9640edbbc46cc1a5685edd6c2e29310?method=download&shareKey=b217663dff94c6cdd4e60ab9bde9495e"/>
</div>

#### 优点：

- **动量的存在使得 SGD 算法在训练的过程中更加的稳定。**

#### 缺点：

- **没有解决SGD容易陷入局部最优的问题**



# NAG：SGD with Nesterov Acceleration

**SGD 还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。 NAG全称Nesterov Accelerated Gradient，是在SGD、SGD-M的基础上的进一步改进，改进点在于定义1。我们知道在时刻t的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。因此，NAG在步骤1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：**

<div align=center>
<img src="https://note.youdao.com/yws/api/personal/file/WEB9ef2842175c1f24583811fea3a1092e7?method=download&shareKey=1dac3f5c3caed30b7b23984e328ced99"/>
</div>

#### 优点：

- **收敛快**
- **在一定程度上解决了陷入局部最优的问题**

#### 缺点：

- **学习率未自适应**



# AdaGrad

**此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。**

<div align=center>
<img src="https://note.youdao.com/yws/api/personal/file/WEB6eb3fa9496f2a93daa8415c0e060e2dc?method=download&shareKey=a964b8c14be1170b9227a074338d8ae1"/>
</div>

#### 优点：

- **引入二阶动量可以对不同参数进行不同速度的改变，达到自适应学习率的目的**

#### 缺点：

- **由于二阶动量是单调递增的，所以会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。**



# AdaDelta / RMSProp

**由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。修改的思路很简单。前面我们讲到，指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：**

<div align=center>
<img src="https://note.youdao.com/yws/api/personal/file/WEBae7f0a96d040ed1fbd3517c9a2882bfe?method=download&shareKey=6f3dbe5f77e0003c334aa3033b5548e6"/>
</div>

#### 优点：

- **使用加权平均使得学习率不会单调递减至0，可以加快收敛速度。**



# Adam

**Adam和Nadam的出现就很自然而然了——它们是前述方法的集大成者。我们看到，SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。**

<div align=center>
<img src="https://note.youdao.com/yws/api/personal/file/WEBfc1fb5fafb88871f39f8165ca7e3186d?method=download&shareKey=90320e976517042023538b994d978c65"/>
</div>


# Nadam

**Adam是集大成者，但它居然遗漏了Nesterov, Nadam就是加上了Nesterov按照NAG的步骤1：**

<div align=center>
<img src="https://note.youdao.com/yws/api/personal/file/WEB562c5c2c473692dbfce42603302134f0?method=download&shareKey=070d72cf6682ee25d981d92f8ab69178"/>
</div>