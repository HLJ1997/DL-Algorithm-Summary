# 机器学习中的正则项

## 概述：

**在机器学习或深度学习过程中，我们的目的是学习一个模型或则函数*f*，使得代价函数*L*在训练集中最小。这就涉及到一个问题，由于损失函数只考虑在训练集上的最小值，称之为经验风险最小化，由于训练集当中噪声的存在，冗余的特征可能成为过拟合的一种来源。这是因为，对于噪声，模型无法从有效特征当中提取信息进行拟合，故而会转向冗余特征。**

<div align=center>
<img src="https://pic3.zhimg.com/v2-d88d37f41b60d2eb3cbce6d9771b2eed_r.jpg?source=1940ef5c"/>
</div>

**如上图最右边所示，函数为了拟合噪声产生的冗余特征，引入了额外的参数使模型出现了过拟合现象。为了对抗过拟合，我们需要向损失函数中加入描述模型复杂程度的正则项将经验风险最小化问题转化为结构风险最小化。正则化是结构风险最小化策略的实 现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。**

## 正则项：

模型选择如下式子所示：
$$
f^* = \min _ { f \in F } \frac { 1 } { N } \sum _ { i = 1 } ^ { N } L ( y _ { i } , f ( x _ { i } ) ) + \lambda J ( f )\\
x_{i},y_{i}分别是训练样本的特征的标签，特征可以是D维的即x_{i} = [{x_{i}^{1},x_{i}^{2},...,x_{i}^{D}}],N为训练样本的个数，深度学习中一般为mini-batch的大小，\\L是损失函数，f是模型由D个参数构成:W=[{w^{1},w^{2},...,w^{D}}],J是正则项,\lambda是常数。
$$

#### L0正则项：

$$
f^* = \min _ { f \in F } \frac { 1 } { N } \sum _ { i = 1 } ^ { N } L ( y _ { i } , f ( x _ { i } ) ) + \lambda ||W||_{0}\\
通过引入L0正则，实际上是向优化过程引入了一种惩罚机制：当优化算法希望增加模型复杂度（此处特指将原来为零的参数更新为非零的情形）\\以降低模型的经验风险（即降低全局损失）时，在结构风险上进行大小为\lambda||W||_{0}的的惩罚，当增加模型复杂度在经验风险上的收益不足时整个结\\构风险实际上会增大而非减小。因此优化算法会拒绝此类更新。
$$



**引入L0正则项会使模型更加稀疏，以及使得模型易于解释，但L0正则项也有无法避免的问题：非连续、非凸、不可微。因此，在引入L0正则项的目标函数无法在方向传播中进行优化。**



#### L1正则项目：

$$
f^* = \min _ { f \in F } \frac { 1 } { N } \sum _ { i = 1 } ^ { N } L ( y _ { i } , f ( x _ { i } ) ) + \lambda ||W||_{1}\\
通过引入L0正则项类似，引入L1正则项是在结构风险上进行大小为\lambda|W|的惩罚，以达到稀疏化的目的。
$$

#### L2正则项目：

**L1正则项能够到达抗过拟合并使参数空间更稀疏，但是L1正则项是不连续的，所以在参数空间内不可求导，所以引入L2正则项:**
$$
f^* = \min _ { f \in F } \frac { 1 } { N } \sum _ { i = 1 } ^ { N } L ( y _ { i } , f ( x _ { i } ) ) + \lambda ||W||_{2}\\
$$

#### L1正则与L2正则对比：

- **L1正则鲁棒性更强，对异常值更不敏感**。

- **L2正则较L1正则运算更方变**

- **L1正则比L2正则更容易稀疏：**
  $$
  \frac{dL_{1}}{dw}=sign(w)\\
  \frac{dL_{2}}{dw}=w
  $$
  

<div align=center>
<img src="https://pic2.zhimg.com/v2-ea5454fc80e6c601fdb347caeeba28cc_r.jpg?source=1940ef5c"/>
</div>

<div align=center>
<img src="https://pic3.zhimg.com/v2-d88e01e730c8e8d91facbd2b1248bfec_r.jpg?source=1940ef5c"/>
</div>

​	**于是会发现，在梯度更新时，不管 L1 的大小是多少（只要不是0）梯度都是1或者-1，所以每次更新时，它都是稳步向0前进。**

​	**而看 L2 的话，就会发现它的梯度会越靠近0，就变得越小。也就是说加了 L1 正则的话基本上经过一定步数后很可能变为0，**

​	**而 L2 几乎不可能，因为在值小的时候其梯度也会变小。于是也就造成了 L1 输出稀疏的特性。**